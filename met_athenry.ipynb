{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><img align=\"right\" width=\"350\" src=\"img/ATU-Logo-Full-RGB-Green.jpg\"> Programming for Data Analysis - Project 1\n",
    "</h1>\n",
    "<p> \n",
    "Course: HDip in Computing in Data Analytics <br>\n",
    "Module: Programming for Data Analysis <br>\n",
    "Lecturer: Brian McGinley <br>\n",
    "Project: Project 1 for the Programming for Data Analysis module of the HDip in Data Analytics beginning January 2023. \n",
    "    \n",
    "Student: Eilis Donohue (G00006088)\n",
    "\n",
    "Project spec:\n",
    "See ProgDAProject.pdf\n",
    "\n",
    "Software Used: \n",
    " - Python v3.10 and higher\n",
    " - Jupyter Notebook 6.5.2   \n",
    " </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Intro)\n",
    "- [Dataset Ingest and Preparation](#Prep)\n",
    "- [Preliminary Analysis](#Prelim)\n",
    "- [Overall Variable Correlation](#Corr)\n",
    "- [Rainfall Analysis](#Rain)\n",
    "- [Pressure Analysis](#Pressure)\n",
    "- [Temperature Analysis](#Temperature)\n",
    "- [Wind Speed Analysis](#Wind)\n",
    "- [Data Synthesisation](#Synth)\n",
    "- [Conclusions](#Conclusions)\n",
    "- [References](#Refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"Intro\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span>\n",
    "\n",
    "The historical meteorological data from the weather station at Athenry, Co. Galway has been downloaded for 10 years from the met.ie website [1]. This dataset comprises a number of meteological variables as previewed below. The raw dataset (hly1875.csv) and its metadata is stored in the \\data\\Athenry_met folder. For the purposes of this study, the following variables have been chosen for analysis and data synthesisation.\n",
    "\n",
    "- Rainfall\n",
    "- Air Temperature\n",
    "- Mean sealevel pressure\n",
    "- Mean Wind speed\n",
    "\n",
    "Each row of the raw dataset is an hourly recording of each variable.  This study is summarised as follows:\n",
    "\n",
    "1. A preliminary analysis of the dataset (4 variables above) is performed to gain some insight into the overall trends and data distribution.\n",
    "2. To simplify the data and its synthesisation, a daily aggregation of the variables is performed based on mean (and daily summation in the case of rainfall).\n",
    "3. As weather is seasonal, a season variable is assigned to the data to allow for potentially more accurate distribution fitting and data synthesisation.\n",
    "4. Each of the four variables above is analysed and a distribution is fitted based on curve fitting to the real data histograms. For some distributions, **Scipy stats** and **curve_fit** is used to generate fitted distributions and their parameters. The most appropriate distribution for each dataset based on a visual check.\n",
    "5. Data is synthesised (for 1 year) using appropriate **Numpy** random number generators models using the distribution features found in step 4. \n",
    "6. The synthesised data is outputted to a csv file (synthesised_met_data.csv) in /data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Ingest and Preparation  <a id=\"Prep\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span>\n",
    "\n",
    "This section has the code used to ingest the data csv file and do some preparation of the data (i.e., convert the numeric fields to floats and remove blankspaces). The following steps are taken to prepare the data:\n",
    "\n",
    "1. The data is read in to a **Pandas** dataframe.\n",
    "2. The 4 variables of interest are isolated and copied to another dataframe.\n",
    "3. The timestamp in the dataset is converted to a datetime object. The data is indexed by datetime as appropriate.  \n",
    "4. Weather is inherently seasonal, so a new variable 'season' is added to the dataframe based on the month (i.e., meteorological winter is December to February inclusive, and so on).\n",
    "5. The data is reduced to a daily aggregation based on the daily mean or sum (in the case of rainfall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, rayleigh, lognorm, multivariate_normal, expon, weibull_min\n",
    "from scipy.optimize import *\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some function definitions\n",
    "# Returns a season designation based on month to a dataframe \n",
    "def split_seasons(df_to_split, months):\n",
    "  season_df = pd.DataFrame()\n",
    "  for month in months:\n",
    "    month_loc = df_to_split['datetime'].dt.month.between(month, month)\n",
    "    season_df = pd.concat([season_df, df_to_split.loc[month_loc]])\n",
    "  return season_df\n",
    "\n",
    "# Returns Pearsons correlation on dataframe\n",
    "def get_corr(data):\n",
    "    return data.corr()\n",
    "\n",
    "# assigns a season class to the data to allow for referencing of data without splitting dataframe\n",
    "def assign_season_class(the_data, the_class, months):\n",
    "  for month in months:\n",
    "    month_loc = the_data['datetime'].dt.month.between(month, month)\n",
    "    the_data.loc[month_loc, 'season'] = the_class\n",
    "  return the_data\n",
    "\n",
    "# function to find the midpoints of a array of bins (returned from plt.hist())\n",
    "def mean_bins(given_bins):\n",
    "  bins = []\n",
    "  for i in range(len(given_bins[:-1])):\n",
    "    bins.append((given_bins[i]+given_bins[i+1])/2)\n",
    "  return np.array(bins)\n",
    "\n",
    "# Define a styler for tables \n",
    "def my_styler(df, precision, caption):\n",
    "    df_styler = df.style.format(precision=precision).set_caption(caption)\n",
    "    return df_styler \n",
    "\n",
    "# Highlights values in a table between left and right values. Returns a styler object that can be displayed\n",
    "def styler_highlight_between(df_styler, left, right):\n",
    "    print(f\"{left} {right}\")\n",
    "    df_styler = df_styler.highlight_between(color=\"#fffd75\", left=left, right=right, inclusive='both') \n",
    "    return df_styler\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full met.ie dataset (downloaded from https://www.met.ie/climate/available-data/historical-data)\n",
    "all_data_df = pd.read_csv(\"data/Athenry_met/hly1875.csv\", skiprows=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return all the columns of the raw dataset\n",
    "all_data_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to be datetime\n",
    "all_data_df['datetime'] = pd.to_datetime(all_data_df['date'])\n",
    "all_data_df = all_data_df.set_index(all_data_df['datetime'])\n",
    "all_data_df.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the data since 2012 to end 2022\n",
    "time_start = \"2012-01-01 00:00:00\"\n",
    "time_end = \"2022-12-31 23:00:00\"\n",
    "# data_df is the time period to analyse\n",
    "data_df = all_data_df.loc[time_start : time_end].copy()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blankspace with nan [2]\n",
    "data_df = data_df.replace(r'^\\s+$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 4 met variables - hourly rainfall, temperature, sealevel pressure and wind speed\n",
    "# Convert the 4 chosen variables to float types\n",
    "data_df[['rain', 'temp', 'msl', 'wdsp']] = data_df[['rain', 'temp', 'msl', 'wdsp']].astype(float)\n",
    "# Create a smaller dataframe with the required variables\n",
    "mydata_df = data_df[['rain', 'temp', 'msl', 'wdsp', 'datetime']].copy()\n",
    "\n",
    "\n",
    "mydata_df_i = mydata_df.copy()\n",
    "winter_months = [12, 1, 2]\n",
    "spring_months = [3, 4, 5]\n",
    "summer_months = [6, 7, 8]\n",
    "autumn_months = [9, 10, 11]\n",
    "\n",
    "# assign a new variable \"season\" to the data\n",
    "mydata_df_i['season'] = ''\n",
    "mydata_df_i = assign_season_class(mydata_df_i, 'winter', winter_months)\n",
    "mydata_df_i = assign_season_class(mydata_df_i, 'spring', spring_months)\n",
    "mydata_df_i = assign_season_class(mydata_df_i, 'summer', summer_months)\n",
    "mydata_df_i = assign_season_class(mydata_df_i, 'autumn', autumn_months)\n",
    "print(mydata_df_i.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a pairplot of the data for 3 years\n",
    "sns.set_theme(style='whitegrid')\n",
    "# take 3 years of data for the plot\n",
    "data_3years_h = mydata_df_i.loc[mydata_df_i['datetime'].between('2020-01-01', '2020-12-31')]\n",
    "g = sns.pairplot(data_3years_h, hue=\"season\", diag_kind=\"kde\")\n",
    "g.map_lower(sns.kdeplot, levels=7, color=\".2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the correlation table for the entire dataset\n",
    "corr_styler_list = []\n",
    "df_corr =  get_corr(mydata_df_i.drop(columns = [\"season\", \"datetime\"]))    \n",
    "df_styler = my_styler(df_corr, 2,  \"Correlation - all data\")\n",
    "df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "corr_styler_list = [df_styler]\n",
    "class_names = ['winter', 'spring', 'summer', 'autumn']  # to allow looping over seasons\n",
    "\n",
    "# Get correlation matrices for the 4 seasons and display\n",
    "for i, item in enumerate(class_names):\n",
    "    # Extract the data related to one season\n",
    "    season_data = mydata_df_i[mydata_df_i[\"season\"] == item].copy()\n",
    "    # Strip the class column before passing to function\n",
    "    season_data.drop(columns = [\"season\", \"datetime\"], inplace=True) \n",
    "    df_corr =  get_corr(season_data)      \n",
    "    df_styler = my_styler(df_corr, 2, item + \" Correlation\")\n",
    "    df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "    df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "    corr_styler_list.append(df_styler)\n",
    "    \n",
    "for item in corr_styler_list:\n",
    "    display(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data based on daily mean or sum (in the case of rainfall)\n",
    "mydata_agg = pd.DataFrame()\n",
    "mydata_agg['Rainfall sum'] = data_df[['rain', 'datetime']].groupby(pd.Grouper(key='datetime', freq='D')).sum()\n",
    "mydata_agg['Temp mean'] = data_df[['temp', 'datetime']].groupby(pd.Grouper(key='datetime', freq='D')).mean()\n",
    "mydata_agg['Pressure mean'] = data_df[['msl', 'datetime']].groupby(pd.Grouper(key='datetime', freq='D')).mean()\n",
    "mydata_agg['Windspeed mean'] = data_df[['wdsp', 'datetime']].groupby(pd.Grouper(key='datetime', freq='D')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to split the data and return the data based on a season\n",
    "# Put datetime back as a variable [3]\n",
    "\n",
    "mydata_agg_i = mydata_agg.reset_index()\n",
    "winter_months = [12, 1, 2]\n",
    "spring_months = [3, 4, 5]\n",
    "summer_months = [6, 7, 8]\n",
    "autumn_months = [9, 10, 11]\n",
    "\n",
    "# assign a new variable \"season\" to the data\n",
    "mydata_agg_i['season'] = ''\n",
    "mydata_agg_i = assign_season_class(mydata_agg_i, 'winter', winter_months)\n",
    "mydata_agg_i = assign_season_class(mydata_agg_i, 'spring', spring_months)\n",
    "mydata_agg_i = assign_season_class(mydata_agg_i, 'summer', summer_months)\n",
    "mydata_agg_i = assign_season_class(mydata_agg_i, 'autumn', autumn_months)\n",
    "print(mydata_agg_i.head(10))\n",
    "\n",
    "class_names = ['winter', 'spring', 'summer', 'autumn']  # to allow looping over seasons\n",
    "var_names = ['Rainfall sum', 'Temp mean', 'Pressure mean', 'Windspeed mean'] # to allow looping over variables\n",
    "plot_titles = ['Rainfall', 'Temperature', 'Pressure', 'Wind Speed']\n",
    "units_names = ['(mm)', '(deg C)', '(hPa)', '(knots)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis <a id=\"Prelim\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "\n",
    "The preliminary analysis of the dataset is carried out to assess the overall statistics and variations of the variables.\n",
    " \n",
    "- Firstly, the histograms of the entire dataset are plotted - this is the distribution of the hourly data. The number of bins was chosen based on iterative process to achieve best balance to minimise noise in the data, and to identify the underlying trends in the data.\n",
    "- The boxplots are used to visualise the main summary statistics of the dataset. The median of each variable is represented by the orange line with the box itself enclosing the middle 50% of the data or interquartile range (from the 25% to the 75% quartile). Seasonal variations can be identified easily. The skew and spread of the data can be qualitatively shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of the 8 year dataset for the 4 chosen variables\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(\"Hourly met data at Athenry (2015-2022 inc)\", fontsize=18)\n",
    "axs[0,0].hist(data_df['rain'], bins=100)\n",
    "axs[0,0].set_xlabel('Rainfall (mm)', fontsize=12)\n",
    "axs[0,1].hist(data_df['temp'], bins=50)\n",
    "axs[0,1].set_xlabel('Temperature (deg)', fontsize=12)\n",
    "axs[1,0].hist(data_df['msl'], bins=50)\n",
    "axs[1,0].set_xlabel('Sealevel Pressure (hPa)', fontsize=12)\n",
    "axs[1,1].hist(data_df['wdsp'], bins=12)\n",
    "axs[1,1].set_xlabel('Wind speed (knots)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up 1 fig with 4 boxplots\n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 8))\n",
    "for i, var in enumerate(var_names):\n",
    "    ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "\n",
    "    # create list of data to do a boxplot\n",
    "    plot_data = [mydata_agg_i[var].loc[mydata_agg_i['season'] == 'winter'],\n",
    "                mydata_agg_i[var].loc[mydata_agg_i['season'] == 'spring'],\n",
    "                mydata_agg_i[var].loc[mydata_agg_i['season'] == 'summer'],\n",
    "                mydata_agg_i[var].loc[mydata_agg_i['season'] == 'autumn']]\n",
    "\n",
    "    # whis is arbitrarily high so that the whiskers represent min and max\n",
    "    axs[ax_list[i]].boxplot(plot_data, whis=10000)   \n",
    "    axs[ax_list[i]].set_ylabel(f'{var} {units_names[i]}')\n",
    "    axs[ax_list[i]].set_xticklabels(class_names, fontsize=10)\n",
    "    axs[ax_list[i]].set_title(plot_titles[i], fontsize=12)\n",
    "    axs[ax_list[i]].grid(visible=True, which='both', axis='both')\n",
    "fig.tight_layout() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# troubleshooting pressure for autumn (for some reason, the boxplot will not display for pressure in autumn)\n",
    "\n",
    "plot_data = [mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == 'winter'],\n",
    "                mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == 'spring'],\n",
    "                mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == 'summer'],\n",
    "                mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == 'autumn']]\n",
    "\n",
    "print(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == 'autumn'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot of the data for 3 years\n",
    "sns.set_theme(style='whitegrid')\n",
    "# take 3 years of data for the plot\n",
    "data_3years = mydata_agg_i.loc[mydata_agg_i['datetime'].between('2020-01-01', '2022-12-31')]\n",
    "g = sns.pairplot(data_3years, hue=\"season\", diag_kind=\"kde\")\n",
    "g.map_lower(sns.kdeplot, levels=7, color=\".2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Variable Correlation <a id=\"Corr\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "In order to investigate correlation between the data variables, the Pearson correlation method in pandas is applied to the data (by season).\n",
    "\n",
    "- Overall, there is no very strong correlation between any of the data variables investigated as part of this study. This was inferred above in the pairplot. Correlation values greater or less than 0.3 and -0.3 respectively are highlighted in the tables below.  \n",
    "\n",
    "- The strongest correlations are between pressure and windspeed and pressure and rainfall. The correlation between pressure and windspeed ranges between -0.37 and -0.46 and ranges from from -0.49 to -0.42 between pressure and rainfall.   \n",
    "\n",
    "- Winter shows the highest degree of correlation between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the correlation table for the entire dataset\n",
    "corr_styler_list = []\n",
    "df_corr =  get_corr(mydata_agg_i.drop(columns = [\"season\", \"datetime\"]))    \n",
    "df_styler = my_styler(df_corr, 2,  \"Correlation - all data\")\n",
    "df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "corr_styler_list = [df_styler]\n",
    "\n",
    "# Get correlation matrices for the 4 seasons and display\n",
    "for i, item in enumerate(class_names):\n",
    "    # Extract the data related to one season\n",
    "    season_data = mydata_agg_i[mydata_agg_i[\"season\"] == item].copy()\n",
    "    # Strip the class column before passing to function\n",
    "    season_data.drop(columns = [\"season\", \"datetime\"], inplace=True) \n",
    "    df_corr =  get_corr(season_data)      \n",
    "    df_styler = my_styler(df_corr, 2, item + \" Correlation\")\n",
    "    df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "    df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "    corr_styler_list.append(df_styler)\n",
    "    \n",
    "for item in corr_styler_list:\n",
    "    display(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainfall Analysis <a id=\"Rain\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "### Rainfall data analysis\n",
    "- The rainfall is highly skewed towards values of zero. The data here may be seen as split into two regimes - the time or days when there is no rain and then those times or days when there is rain as is done here [4].\n",
    "- It is useful to remove the 0 rainfall days by imposing a 1mm limit to determine a \"rainy day\" v \"dry day\". This approach is used here [4].\n",
    "- Rainfall over a period of time can follow an exponential distribution [4] when the 0 rainfall periods are removed. The exponential distribution is shown below and appears to be a good fit to the \"rainy day\" data [6].  \n",
    "\n",
    "The exponential distribution:\n",
    "\n",
    "$$\n",
    "f(x;\\lambda) = \n",
    "\\begin{cases} \n",
    "\\lambda e^{-\\lambda x} & \\text{if } x \\geq 0 \\\\\n",
    "0 & \\text{if } x < 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some analysis on the daily aggregated rainfall data over 10 years.\n",
    "rainfall_df = mydata_agg_i[['Rainfall sum', 'datetime', 'season']]\n",
    "rainfall_df = rainfall_df.set_index('datetime')\n",
    "max_rain = rainfall_df['Rainfall sum'].max()\n",
    "min_rain = rainfall_df['Rainfall sum'].min()\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10));\n",
    "# list of axes for looping\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "fig.suptitle('Rainfall data normalised distribution with exponential curve fit', fontsize=16)\n",
    "\n",
    "# Define empty lists to store the rainfall params and frequency from the curve fit for data synthesisation later\n",
    "rainfall_exp_params_season = []\n",
    "fw_seasonal = []\n",
    "for i, season in enumerate(class_names):\n",
    "  rainfall_df_season = pd.DataFrame()\n",
    "  rainfall_df_season = rainfall_df[rainfall_df['season'] == season].drop(columns=['season'])\n",
    "  # frequency of rainy days with criterion of >1mm rainfall in a day\n",
    "  fw = (rainfall_df_season['Rainfall sum']>1).sum() / len(rainfall_df_season)\n",
    "  fw_seasonal.append(fw)\n",
    "  daily_rain_rainydays = rainfall_df_season[rainfall_df_season['Rainfall sum']>1]\n",
    "  # defining the bin size\n",
    "  bin_size = 2\n",
    "  # find the min and max of the data\n",
    "  daily_rain_rainydays_floor = math.floor(daily_rain_rainydays['Rainfall sum'].min())\n",
    "  daily_rain_rainydays_ceiling = math.ceil(daily_rain_rainydays['Rainfall sum'].max())\n",
    "\n",
    "  # generate a list of bins based on the above to fit the data\n",
    "  bins = np.arange(daily_rain_rainydays_floor, daily_rain_rainydays_ceiling+bin_size, bin_size)\n",
    "\n",
    "  # returns the normalised data for the histogram (area under curve =1)\n",
    "\n",
    "  n, bins_returned, _ = axs[ax_list[i]].hist(daily_rain_rainydays['Rainfall sum'], bins=bins, density=True)\n",
    "  bin_start = np.array(bins[:-1])\n",
    "\n",
    "  # Use curve_fit method as in: https://stackoverflow.com/questions/50448199/lognormal-curve-fit\n",
    "  # PDF for the exponential distribution - so that curve fitting can be done\n",
    "  def f_exp(bin_start, lamda_1):\n",
    "    return lamda_1*np.exp(-1*lamda_1*bin_start)\n",
    "\n",
    "  # returns the exponential distribution parameters for the data fit\n",
    "  params, extras = curve_fit(f_exp, bin_start, n)\n",
    "\n",
    "  # fit a exponential curve\n",
    "  lamda_1 = params[0]\n",
    "  pdf_expon = (lamda_1*np.exp(-1*lamda_1*bin_start))\n",
    "  rainfall_exp_params_season.append(lamda_1)\n",
    "\n",
    "  axs[ax_list[i]].plot(bin_start, pdf_expon, 'r', label='Exponential PDF')\n",
    "  axs[ax_list[i]].set_xlabel(f'Daily Rainfall Sum (mm)', fontsize=10)\n",
    "  axs[ax_list[i]].set_ylabel(f'Normalised histogram', fontsize=10)\n",
    "  axs[ax_list[i]].set_title(season, fontsize=14)\n",
    "  axs[ax_list[i]].grid(visible=True, which='both', axis='both')\n",
    "axs[(0,0)].legend();\n",
    "fig.tight_layout()\n",
    "fig, ax = plt.subplots(1, figsize=(3, 4));\n",
    "ax.bar(class_names, fw_seasonal);\n",
    "ax.set_title(\"Frequency of rainy days (fw)\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a linear regression plot between rainfall (>1mm) and pressure \n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 5))\n",
    "\n",
    "axs[0].scatter(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], mydata_agg_i['Rainfall sum'].loc[mydata_agg_i['Rainfall sum']>1]);\n",
    "res = stats.linregress(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], mydata_agg_i['Rainfall sum'].loc[mydata_agg_i['Rainfall sum']>1], alternative='two-sided')\n",
    "print(res)\n",
    "axs[0].plot(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], res.intercept + res.slope*mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], 'r', label='linear fitted line')\n",
    "axs[0].set_ylabel(\"Daily Rainfall (mm)\")\n",
    "axs[0].set_xlabel(\"Mean Pressure (hPa)\")\n",
    "axs[0].legend()\n",
    "\n",
    "#r = np.corrcoef(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], mydata_agg_i['Rainfall sum'].loc[mydata_agg_i['Rainfall sum']>1])\n",
    "#print(r)\n",
    "\n",
    "# Get a linear regression plot between log(rainfall) (>1mm) and pressure \n",
    "\n",
    "res_log = stats.linregress(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], np.log(mydata_agg_i['Rainfall sum'].loc[mydata_agg_i['Rainfall sum']>1]), alternative='two-sided')\n",
    "print(f'Log linear fit {res_log}')\n",
    "axs[1].scatter(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], np.log(mydata_agg_i['Rainfall sum'].loc[mydata_agg_i['Rainfall sum']>1]));\n",
    "axs[1].plot(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], res_log.intercept + res_log.slope*mydata_agg_i['Pressure mean'].loc[mydata_agg_i['Rainfall sum']>1], 'r', label='loglinear fitted line')\n",
    "axs[1].set_ylabel(\"Log(Daily Rainfall) (mm)\")\n",
    "axs[1].set_xlabel(\"Mean Pressure (hPa)\")\n",
    "axs[1].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a linear regression plot between windspeed and pressure \n",
    "fig, axs = plt.subplots(2,2, figsize=(12, 12))\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "\n",
    "# Define empty lists to store the seasonal linear regression results to a list\n",
    "wind_pressure_linregress_season = []\n",
    "\n",
    "for i, season in enumerate(class_names):\n",
    "\n",
    "#mydata_agg_i[var].loc[mydata_agg_i['season'] == 'winter'\n",
    "  axs[ax_list[i]].scatter(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == season], mydata_agg_i['Windspeed mean'].loc[mydata_agg_i['season'] == season])\n",
    "  res_ws = stats.linregress(mydata_agg_i['Pressure mean'].loc[mydata_agg_i['season'] == season], mydata_agg_i['Windspeed mean'].loc[mydata_agg_i['season'] == season])\n",
    "  wind_pressure_linregress_season.append((res_ws.slope, res_ws.intercept))\n",
    "  \n",
    "  print(res_ws)\n",
    "\n",
    "  axs[ax_list[i]].plot(mydata_agg_i['Pressure mean'], res_ws.intercept + res_ws.slope*mydata_agg_i['Pressure mean'], 'r', label='linear fitted line')\n",
    "  axs[ax_list[i]].set_ylabel(\"Mean windspeed (Kts)\")\n",
    "  axs[ax_list[i]].set_xlabel(\"Mean Pressure (hPa)\")\n",
    "  axs[ax_list[i]].legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pressure Analysis <a id=\"Pressure\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "The seasonal mean sea level pressure is plotted below and a log normal distribution is fitted to the data.\n",
    "\n",
    "- From the preliminary analysis above, the distribution of pressure is skewed to the left.\n",
    "- Trying a lognormal normal distribution seems to provide a reasonably accurate fit to the data. There is some underestimation at lower values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store the pressure params and frequency from the curve fit for data synthesisation later\n",
    "pressure_params_season = []\n",
    "pressure_df = mydata_agg_i[['Pressure mean', 'datetime', 'season']]\n",
    "pressure_df = pressure_df.set_index('datetime')\n",
    "# \n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10));\n",
    "fig.suptitle('Rainfall data with lognormal curve fit', fontsize=16)\n",
    "# Axis list for looping over axes below\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "  \n",
    "for i, season in enumerate(class_names):\n",
    "  pressure_df_season = pd.DataFrame()\n",
    "  pressure_df_season = pressure_df[pressure_df['season'] == season].drop(columns=['season'])\n",
    "  \n",
    "  # defining the bin size\n",
    "  bin_size = 5\n",
    "  # find the min and max of the data\n",
    "  data_floor = math.floor(pressure_df_season['Pressure mean'].min())\n",
    "  data_ceiling = math.ceil(pressure_df_season['Pressure mean'].max())\n",
    "\n",
    "  # Generate a list of bins based on the min and max above to fit the data\n",
    "  bins = np.arange(data_floor, data_ceiling+bin_size, bin_size)\n",
    "  # Returns the normalised data for the histogram (area under curve =1)\n",
    "  n, bins_returned, _ = axs[ax_list[i]].hist(pressure_df_season['Pressure mean'], bins=bins, density=True)\n",
    "  \n",
    "  # Gets the mid-point of the bins\n",
    "  bin_mid = mean_bins(bins_returned)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Log-normal_distribution\n",
    "# This could be moved into functions section \n",
    "  def f_lognormal(x, mu, sigma):\n",
    "    return 1/(np.sqrt(2*np.pi)*sigma*x)*np.exp(-((np.log(x)-mu)**2)/(2*sigma**2))\n",
    "\n",
    "  # returns mu and sigma in params \n",
    "  params, extras = curve_fit(f_lognormal, bin_mid, n)\n",
    "  mu_pressure = params[0]\n",
    "  sigma_pressure = params[1]\n",
    "  # store tuples to this list for use in the data synth later\n",
    "  pressure_params_season.append((mu_pressure, sigma_pressure))\n",
    "  # fit a lognormal curve\n",
    "  pdf_lognormal = f_lognormal(bin_mid, mu_pressure, sigma_pressure)\n",
    "\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_lognormal, 'r', label='Lognormal PDF')\n",
    "  axs[ax_list[i]].set_xlabel(f'Daily Mean Pressure (hPa)', fontsize=10)\n",
    "  axs[ax_list[i]].set_ylabel(f'Normalised histogram', fontsize=10)\n",
    "  axs[ax_list[i]].set_title(season, fontsize=14)\n",
    "  axs[ax_list[i]].grid(visible=True, which='both', axis='both')\n",
    "axs[(0,0)].legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Data Analysis <a id=\"Temperature\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "Qualitatively, the preliminary analysis of temperature distribution above shows a broadly normal distribution.  The normal distribution PDF is given below.\n",
    "A Log normal PDF curve was also fitted. The normal and log normal fits are almost coincident for winter, spring and autumn which shows that there is no significant skew in the data and therefore, a normal distribution is fitted. The data for summer however, shows a slight skew to the left, so the log normal fit parameters will be taken for summer. \n",
    "\n",
    "The normal distribution:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\big( \\frac{x- \\mu}{\\sigma} \\big)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store the temperature params and frequency from the curve fit for data synthesisation later\n",
    "temp_params_season_normal = []\n",
    "temp_params_season_lognormal = []\n",
    "temp_df = mydata_agg_i[['Temp mean', 'datetime', 'season']]\n",
    "temp_df = temp_df.set_index('datetime')\n",
    "# \n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10));\n",
    "fig.suptitle('Temperature data normalised distribution with normal curve fit', fontsize=16)\n",
    "# Axis list for looping over axes below\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "  \n",
    "for i, season in enumerate(class_names):\n",
    "  temp_df_season = pd.DataFrame()\n",
    "  temp_df_season = temp_df[temp_df['season'] == season].drop(columns=['season'])\n",
    "  \n",
    "  # defining the bin size\n",
    "  bin_size = 1\n",
    "  # find the min and max of the data\n",
    "  data_floor = math.floor(temp_df_season['Temp mean'].min())\n",
    "  data_ceiling = math.ceil(temp_df_season['Temp mean'].max())\n",
    "\n",
    "  # Generate a list of bins based on the min and max above to fit the data\n",
    "  bins = np.arange(data_floor, data_ceiling+bin_size, bin_size)\n",
    "  # Returns the normalised data for the histogram (area under curve =1)\n",
    "  n, bins_returned, _ = axs[ax_list[i]].hist(temp_df_season['Temp mean'], bins=bins, density=True)\n",
    "  \n",
    "  # Gets the mid-point of the bins\n",
    "  bin_mid = mean_bins(bins_returned)\n",
    "  mu_temp = temp_df_season['Temp mean'].mean()\n",
    "  sigma = temp_df_season['Temp mean'].std()\n",
    "  # Getting curve fits this time with scipy stats\n",
    "  param_normal = norm.fit(temp_df_season['Temp mean'])\n",
    "  print(param_normal)\n",
    "  param_lognormal = lognorm.fit(temp_df_season['Temp mean'])\n",
    "  print(param_lognormal)\n",
    "  # plotting the fitted pdf curves for normal and log normal\n",
    "  pdf_fitted_norm = norm.pdf(bin_mid, loc=param_normal[0], scale=param_normal[1])\n",
    "  pdf_fitted_lognorm = lognorm.pdf(bin_mid, param_lognormal[0], param_lognormal[1], param_lognormal[2])\n",
    "  # store tuples to this list for use in the data synth later\n",
    "  temp_params_season_normal.append((param_normal[0], param_normal[1]))\n",
    "  temp_params_season_lognormal.append((param_lognormal[0], param_lognormal[1], param_lognormal[2]))\n",
    "  # fit a normal curve\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_norm, 'r', label='Normal PDF')\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_lognorm, 'g', label='Lognormal PDF')\n",
    "  \n",
    "  axs[ax_list[i]].set_xlabel(f'Daily Mean Temp (deg C)', fontsize=10)\n",
    "  axs[ax_list[i]].set_ylabel(f'Normalised histogram', fontsize=10)\n",
    "  axs[ax_list[i]].set_title(season, fontsize=14)\n",
    "  axs[ax_list[i]].grid(visible=True, which='both', axis='both')\n",
    "axs[(0,0)].legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Speed Data Analysis <a id=\"Wind\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span> \n",
    "\n",
    "The preliminary analysis shows mean wind speed to have a tail for higher values. This distribution does not look normal or indeed log normal as the temperature and pressure above does.  A rayleigh distribution is often used to model wind speed, however, Weibull is also used [12] for modelling extreme values.\n",
    "\n",
    "The data below seems well represented by a mixture of Log normal (summer and autumn) and Rayleigh (winter and spring). Generally, the log normal and Rayleigh distributions are a better fit to the data than the normal distribution which fails to capture the extreme values as accuractly as Rayleigh and Log normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store the wind params and frequency from the curve fit for data synthesisation later\n",
    "wind_params_season_normal = []\n",
    "wind_params_season_lognormal = []\n",
    "wind_params_season_rayleigh = []\n",
    "# isolate the wind data\n",
    "wind_df = mydata_agg_i[['Windspeed mean', 'datetime', 'season']]\n",
    "wind_df = wind_df.set_index('datetime')\n",
    "# \n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10));\n",
    "fig.suptitle('Wind speed data normalised distribution with curve fits', fontsize=16)\n",
    "# Axis list for looping over axes below\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "  \n",
    "for i, season in enumerate(class_names):\n",
    "  wind_df_season = pd.DataFrame()\n",
    "  wind_df_season = wind_df[wind_df['season'] == season].drop(columns=['season'])\n",
    "  \n",
    "  # defining the bin size\n",
    "  bin_size = 1\n",
    "  # find the min and max of the data\n",
    "  data_floor = math.floor(wind_df_season['Windspeed mean'].min())\n",
    "  data_ceiling = math.ceil(wind_df_season['Windspeed mean'].max())\n",
    "\n",
    "  # Generate a list of bins based on the min and max above to fit the data\n",
    "  bins = np.arange(data_floor, data_ceiling+bin_size, bin_size)\n",
    "  # Returns the normalised data for the histogram (area under curve =1)\n",
    "  n, bins_returned, _ = axs[ax_list[i]].hist(wind_df_season['Windspeed mean'], bins=bins, density=True)\n",
    "  \n",
    "  # Gets the mid-point of the bins\n",
    "  bin_mid = mean_bins(bins_returned)\n",
    "  mu_wind = wind_df_season['Windspeed mean'].mean()\n",
    "  sigma = wind_df_season['Windspeed mean'].std()\n",
    "  # Getting curve fits this time with scipy stats\n",
    "  param_normal = norm.fit(wind_df_season['Windspeed mean'])\n",
    "  param_lognormal = lognorm.fit(wind_df_season['Windspeed mean'])\n",
    "  param_rayleigh = rayleigh.fit(wind_df_season['Windspeed mean'])\n",
    "  print(param_lognormal)\n",
    "  param_weibull = weibull_min.fit(wind_df_season['Windspeed mean'])\n",
    "  # Generate some fitted curves using scipy stats \n",
    "  pdf_fitted_norm = norm.pdf(bin_mid, loc=param_normal[0], scale=param_normal[1])\n",
    "  pdf_fitted_lognorm = lognorm.pdf(bin_mid, param_lognormal[0], param_lognormal[1], param_lognormal[2])\n",
    "  pdf_fitted_rayleigh = rayleigh.pdf(bin_mid, param_rayleigh[0], param_rayleigh[1])\n",
    "  pdf_fitted_weibull = weibull_min.pdf(bin_mid, param_weibull[0], param_weibull[1], param_weibull[2])\n",
    "  # store tuples to this list for use in the data synth later\n",
    "  wind_params_season_normal.append((param_normal[0], param_normal[1]))\n",
    "  wind_params_season_lognormal.append((param_lognormal[0], param_lognormal[1], param_lognormal[2]))\n",
    "  wind_params_season_rayleigh.append((param_rayleigh[0], param_rayleigh[1]))\n",
    "  # plotting the fitted pdf curves for normal and log normal\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_norm, 'r', label='Normal PDF')\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_lognorm, 'g', label='Lognormal PDF')\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_rayleigh, 'c', label='Rayleigh PDF')\n",
    "  axs[ax_list[i]].plot(bin_mid, pdf_fitted_weibull, 'm', label='Weibull PDF')\n",
    "  \n",
    "  axs[ax_list[i]].set_xlabel(f'Daily Mean windspeed (Kts)', fontsize=10)\n",
    "  axs[ax_list[i]].set_ylabel(f'Normalised histogram', fontsize=10)\n",
    "  axs[ax_list[i]].set_title(season, fontsize=14)\n",
    "  axs[ax_list[i]].grid(visible=True, which='both', axis='both')\n",
    "axs[(0,0)].legend();\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Synthesisation <a id=\"Synth\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span>\n",
    "\n",
    "In the preceding sections, a collection of PDF fitted curves and associated parameters has been generated based on the dataset daily aggregation.  In this section, the synthesisation of the meteorological values will be attempted, using the fitted parameters for each of the 4 seasons and using Numpy random generators as appropriate.\n",
    "\n",
    "Note that synthesised data will be indicated \"_s\" below.\n",
    "\n",
    "### Rainfall Data Synthesisation\n",
    "Daily rainfall data synthesisation will be carried out as follows:\n",
    "\n",
    "The frequency of rainless days with a frequency of (1-fw) will be used to generate a corresponding number of days in the synthesised data with no rain. \n",
    "- The number of dry days is found by subtracting rainy days from the total number of days and a value of 0 is assigned.\n",
    "- It is possible to generate data for 1 year to synthesise daily rainfall totals by the following:\n",
    "    -- generate some random data based on the exponential distribution parameters which describe the rainfall daily total distribution. The size of the random data is based on the frequency of rainy days, fw, multiplied by the total number of days of required data [14]. \n",
    "- The 2 arrays are joined together and then shuffled randomly.\n",
    "\n",
    "### Pressure Data\n",
    "Pressure data was generated with the numpy.random.lognormal generator.\n",
    "\n",
    "### Temperature Data\n",
    "Temperature data was generated with a numpy.random.normal generator.\n",
    "\n",
    "### Windspeed Data\n",
    "Windspeed data was synthesised twice by two different methods.\n",
    "\n",
    "1. Windspeed data was generated with Numpy Rayleigh random generator \n",
    "2. A seasonal linear regression fit was made on the real data between pressure and windspeed. This linear regression slope and intercept was then applied to the synthesised pressure data along with a tuned noise parameter to synthesis windspeed data.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set up some parameters for the synthesised dataset\n",
    "# no_days is number of rows in synthesised dataset\n",
    "no_years = 5\n",
    "var_names_synth = ['Rainfall sum_s', 'Temp mean_s', 'Pressure mean_s', 'Windspeed mean_s', 'Windspeed linregress_s', 'season'] # to allow looping over variables\n",
    "# empty dataframe for storing the synthesised data\n",
    "# add column for day number https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe\n",
    "# no of days in each season\n",
    "season_days = [31+31+28, 31+30+31, 30+31+31, 30+31+30]\n",
    "met_data_synth = pd.DataFrame(index=range(365*no_years), columns=var_names_synth)\n",
    "print(met_data_synth)\n",
    "# set up a pdf fit dictionary for temperature and wind\n",
    "temp_fit_dict = {'winter':'normal', 'spring':'normal', 'summer':'normal', 'autumn':'normal'}\n",
    "wind_fit_dict = {'winter':'rayleigh', 'spring':'rayleigh', 'summer':'rayleigh', 'autumn':'rayleigh'}\n",
    "\n",
    "# Assign the seasons to the empty dataframe - in the correct order, tho it shouldn't matter because each year is generated identically\n",
    "start_index = 0\n",
    "\n",
    "for i in range(no_years):\n",
    "  for j, season in enumerate(class_names):\n",
    "    season_class = np.array(season_days[j] * [class_names[j]])\n",
    "    end_index = start_index + len(season_class) -1 \n",
    "    met_data_synth['season'].loc[start_index:end_index] = season_class\n",
    "    start_index = end_index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall synthesisation\n",
    "# set up a random generator for exponential distribution\n",
    "# https://numpy.org/devdocs/reference/random/generated/numpy.random.Generator.exponential.html\n",
    "# generate 1 year of daily rainfall totals - assuming that the frequency of rainy days calculated above\n",
    "start_index = 0\n",
    "for y in range(no_years):\n",
    "  # initialise a numpy array from an empty  list\n",
    "  rainfall_syn_data = np.array([])\n",
    "  for i, season in enumerate(class_names):\n",
    "    # Generate the random rainfall data using the numpy exponential \n",
    "    random_rainfall_season = np.random.default_rng().exponential(1/rainfall_exp_params_season[i], int(season_days[i]*fw_seasonal[i]))\n",
    "    # Get the number of rainy days and non-rainy days\n",
    "    rainfall_days_season = int(fw_seasonal[i] * season_days[i])\n",
    "    non_rainfall_days_season = np.array((season_days[i]-rainfall_days_season)*[0])\n",
    "    rainfall_syn_data_season = np.concatenate((random_rainfall_season, non_rainfall_days_season))\n",
    "    # Mix up the 0 rainfall and rainfall days randomly  \n",
    "    np.random.shuffle(rainfall_syn_data_season)\n",
    "    rainfall_syn_data = np.concatenate((rainfall_syn_data, rainfall_syn_data_season)) \n",
    "  end_index = start_index + 365 - 1 \n",
    "  met_data_synth['Rainfall sum_s'].loc[start_index:end_index] = rainfall_syn_data\n",
    "  start_index = end_index + 1\n",
    "\n",
    "# compare the real data with the synthesised data\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5));\n",
    "fig.suptitle('Rainfall data real v synthesised', fontsize=12)\n",
    "ax.hist(mydata_agg_i['Rainfall sum'], bins=10, density=True, label='real')\n",
    "ax.hist(met_data_synth['Rainfall sum_s'], bins=10, density=True, alpha=0.5, label='synthesised')\n",
    "ax.set_xlabel(f'Rainfall (mm)', fontsize=12)\n",
    "ax.set_ylabel(f'Normalised histogram', fontsize=12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature synthesisation\n",
    "\n",
    "\n",
    "start_index = 0\n",
    "for y in range(no_years):\n",
    "  # initialise a numpy array from an empty  list\n",
    "  temp_syn_data = np.array([])\n",
    "  for i, season in enumerate(class_names):\n",
    "    # Generate the random temp data using the numpy normal or log normal generator as per dictionary defined above\n",
    "    if temp_fit_dict[season] == \"normal\":\n",
    "      random_temp_season = np.random.default_rng().normal(temp_params_season_normal[i][0], temp_params_season_normal[i][1], season_days[i])\n",
    "    elif temp_fit_dict[season] == \"lognormal\":\n",
    "      # slightly different as lognormal params were generated from scipy stats\n",
    "      random_temp_season = np.random.lognormal(temp_params_season_lognormal[i][0], temp_params_season_lognormal[i][1], season_days[i])\n",
    "\n",
    "    temp_syn_data = np.concatenate((temp_syn_data, random_temp_season))\n",
    "  end_index = start_index + 365 - 1 \n",
    "  met_data_synth['Temp mean_s'].loc[start_index:end_index] = temp_syn_data\n",
    "  start_index = end_index + 1  \n",
    "\n",
    "# compare the real data with the synthesised data\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5));\n",
    "fig.suptitle('Temperature data real v synthesised', fontsize=12)\n",
    "ax.hist(mydata_agg_i['Temp mean'], bins=15, density=True, label='real')\n",
    "ax.hist(met_data_synth['Temp mean_s'], bins=15, density=True, alpha=0.5, label='synthesised')\n",
    "ax.set_xlabel(f'Temp (deg C)', fontsize=12)\n",
    "ax.set_ylabel(f'Normalised histogram', fontsize=12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pressure data synthesisation\n",
    "\n",
    "start_index = 0\n",
    "for y in range(no_years):\n",
    "  # initialise a numpy array from an empty list\n",
    "  pressure_syn_data = np.array([])\n",
    "  for i, season in enumerate(class_names):\n",
    "    # Generate the random pressure data using the log normal generator \n",
    "    random_pressure_season = np.random.default_rng().lognormal(pressure_params_season[i][0], pressure_params_season[i][1], season_days[i])\n",
    "    # add the season's data onto the end of the array\n",
    "    pressure_syn_data = np.concatenate((pressure_syn_data, random_pressure_season))\n",
    "  end_index = start_index + 365 - 1 \n",
    "  met_data_synth['Pressure mean_s'].loc[start_index:end_index] = pressure_syn_data\n",
    "  start_index = end_index + 1  \n",
    "  \n",
    "# compare the real data with the synthesised data\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5));\n",
    "fig.suptitle('Pressure data real v synthesised', fontsize=12)\n",
    "ax.hist(mydata_agg_i['Pressure mean'], bins=15, density=True, label='real')\n",
    "ax.hist(met_data_synth['Pressure mean_s'], bins=15, density=True, alpha=0.5, label='synthesised')\n",
    "ax.set_xlabel(f'Pressure (hPa)', fontsize=12)\n",
    "ax.set_ylabel(f'Normalised histogram', fontsize=12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind speed synthesisation - changing to rayleigh for all as issues with lognormal\n",
    "wind_fit_dict = {'winter':'rayleigh', 'spring':'rayleigh', 'summer':'rayleigh', 'autumn':'rayleigh'}\n",
    "start_index = 0\n",
    "for y in range(no_years):\n",
    "  # initialise a numpy array from an empty list\n",
    "  wind_syn_data = np.array([])\n",
    "  for i, season in enumerate(class_names):\n",
    "    random_wind_season = np.array([])\n",
    "    # Generate the random temp data using the numpy normal or log normal generator as per dictionary defined above\n",
    "    if wind_fit_dict[season] == \"lognormal\":\n",
    "      mean = np.log(wind_params_season_lognormal[i][2])\n",
    "      #print(mean)\n",
    "      std_dev = wind_params_season_lognormal[i][0]\n",
    "      #print(std_dev)\n",
    "      random_wind_season = np.random.default_rng().lognormal(wind_params_season_lognormal[i][0], wind_params_season_lognormal[i][1], season_days[i])\n",
    "      #random_wind_season = np.random.default_rng().lognormal(mean, std_dev, season_days[i])\n",
    "      #print(random_wind_season)\n",
    "    elif wind_fit_dict[season] == \"rayleigh\":\n",
    "      mode = np.sqrt(2 / np.pi) * wind_params_season_rayleigh[i][1]\n",
    "      mode = mode +1\n",
    "      random_wind_season = np.random.rayleigh(mode, season_days[i])\n",
    "      #print(wind_params_season_rayleigh[i][0])\n",
    "    wind_syn_data = np.concatenate((wind_syn_data, random_wind_season))\n",
    "    end_index = start_index + 365 - 1 \n",
    "  met_data_synth['Windspeed mean_s'].loc[start_index:end_index] = wind_syn_data\n",
    "  start_index = end_index + 1  \n",
    "\n",
    "# compare the real data with the synthesised data\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5));\n",
    "fig.suptitle('Wind speed data real v synthesised', fontsize=12)\n",
    "ax.hist(mydata_agg_i['Windspeed mean'], bins=15, density=True, label='real')\n",
    "ax.hist(met_data_synth['Windspeed mean_s'], bins=15, density=True, alpha=0.5, label='synthesised')\n",
    "ax.set_xlabel(f'Windspeed (Kts)', fontsize=12)\n",
    "ax.set_ylabel(f'Normalised histogram', fontsize=12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Finally write out the synthesised dataset to a csv file\n",
    "met_data_synth.to_csv(\"data/synthesised_met_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind speed synthesisation - based on linear regression fit from above\n",
    "\n",
    "#wind_pressure_linregress_season\n",
    "start_index = 0\n",
    "# initialise a numpy array from an empty list\n",
    "#wind_syn_data = np.array([])\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(10, 10));\n",
    "fig.suptitle('Wind speed data synthesised with lin regress results', fontsize=16)\n",
    "# Axis list for looping over axes below\n",
    "ax_list = [(0,0), (0,1), (1,0), (1,1)] \n",
    "# tuned noise parameters for each season\n",
    "noise_season = [[10, 5], [9.5,4.5], [7,3.5], [6.2, 2.9]]\n",
    "for i, season in enumerate(class_names):\n",
    "  random_wind_season = np.array([])\n",
    "  # Generate the random temp data using the numpy normal or log normal generator as per dictionary defined above\n",
    "  rng=np.random.default_rng()\n",
    "  # Generate some random noise\n",
    "  noise = ((rng.random(len(met_data_synth['Pressure mean_s'].loc[met_data_synth['season']==season]))*noise_season[i][0])-noise_season[i][1])\n",
    "  # model autumn on summer (no linear regression data for autumn)\n",
    "  if season == \"autumn\":\n",
    "    wind_pressure_linregress_season[3] = wind_pressure_linregress_season[2][0], wind_pressure_linregress_season[2][1]\n",
    "    \n",
    "  met_data_synth['Windspeed linregress_s'].loc[met_data_synth['season']==season] = (met_data_synth['Pressure mean_s'].loc[met_data_synth['season']==season] * wind_pressure_linregress_season[i][0]) + wind_pressure_linregress_season[i][1] + noise\n",
    "  axs[ax_list[i]].hist(mydata_agg_i['Windspeed mean'].loc[mydata_agg_i['season']==season], density=True, label='real')\n",
    "  axs[ax_list[i]].hist(met_data_synth['Windspeed linregress_s'].loc[met_data_synth['season']==season], density=True, alpha=0.5, label='synthesised')   \n",
    "  axs[ax_list[i]].set_xlabel(f'Daily Mean windspeed (Kts)', fontsize=10)\n",
    "  axs[ax_list[i]].set_ylabel(f'Normalised histogram', fontsize=10)\n",
    "  axs[ax_list[i]].set_title(season, fontsize=14)\n",
    "\n",
    "# compare the real data with the synthesised data\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5));\n",
    "fig.suptitle('Wind speed data real v synthesised', fontsize=12)\n",
    "ax.hist(mydata_agg_i['Windspeed mean'], bins=12, density=True, label='real')\n",
    "ax.hist(met_data_synth['Windspeed linregress_s'], bins=12, density=True, alpha=0.5, label='synthesised')\n",
    "ax.set_xlabel(f'Windspeed (Kts)', fontsize=12)\n",
    "ax.set_ylabel(f'Normalised histogram', fontsize=12)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Finally write out the synthesised dataset to a csv file\n",
    "met_data_synth.to_csv(\"data/synthesised_met_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the correlation table for the entire dataset\n",
    "corr_styler_list = []\n",
    "# For some reason, having to force everything to floats\n",
    "met_data_synth[['Rainfall sum_s', 'Temp mean_s', 'Pressure mean_s', 'Windspeed mean_s', 'Windspeed linregress_s']] = met_data_synth[['Rainfall sum_s', 'Temp mean_s', 'Pressure mean_s', 'Windspeed mean_s',  'Windspeed linregress_s']].astype(float)\n",
    "df_corr =  get_corr(met_data_synth.drop(columns = [\"season\"]))    \n",
    "df_styler = my_styler(df_corr, 2,  \"Correlation - all data\")\n",
    "df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "corr_styler_list = [df_styler]\n",
    "\n",
    "# Get correlation matrices for the 4 seasons and display\n",
    "for i, item in enumerate(class_names):\n",
    "    # Extract the data related to one season\n",
    "    season_data = met_data_synth[met_data_synth[\"season\"] == item].copy()\n",
    "    # Strip the class column before passing to function\n",
    "    season_data.drop(columns = [\"season\"], inplace=True) \n",
    "    # For some reason, having to force everything to floats\n",
    "    season_data[['Rainfall sum_s', 'Temp mean_s', 'Pressure mean_s', 'Windspeed mean_s', 'Windspeed linregress_s']] = season_data[['Rainfall sum_s', 'Temp mean_s', 'Pressure mean_s', 'Windspeed mean_s', 'Windspeed linregress_s']].astype(float)\n",
    "    df_corr = get_corr(season_data)      \n",
    "    df_styler = my_styler(df_corr, 2, item + \" Correlation\")\n",
    "    df_styler = styler_highlight_between(df_styler, 0.3, 0.99)\n",
    "    df_styler = styler_highlight_between(df_styler, -0.99, -0.3)\n",
    "    corr_styler_list.append(df_styler)\n",
    "    \n",
    "for item in corr_styler_list:\n",
    "    display(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a id=\"Conclusions\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span>\n",
    "\n",
    "- The dataset which comprised hourly meteorological data (2012 - 2022) was aggregated and analysed on the basis of splitting the data into the 4 meteorological seasons.\n",
    "- There was no strong correlation (>0.7 deemed to be reasonably strong correlation) found between any of the variables in the real dataset (weak correlations were found between pressure and rainfall and pressure and wind speed). The highest correlation value was -0.47 between pressure and rainfall.\n",
    "- For each of the 4 chosen meteorological variables, a suitable distribution was chosen and fitted to the data for each of the 4 seasons. Parameters for PDF generation were stored for each season and each variable\n",
    "- Random data was generated for each of the 4 seasons for each data variable for each season.\n",
    "- To test the random number generation, 1 years worth of data was synthesised with Numpy random distribution generators. The generated histograms were compared to the real dataset.\n",
    "- Good agreement between real and synthesised data was found for pressure and temperature datasets. Both of these are largely normally distributed. The synthesised wind speed data (Rayleigh) was skewed towards lower values.\n",
    "- The correlation values are lower than those of the original dataset as no correlation (such as with linear regression or multivariate analysis) was forced on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a id=\"Refs\"></a> <span style=\"font-size: 8pt;\"> [[TOC]](#toc)</span>\n",
    "1. Met.ie dataset source: https://www.met.ie/climate/available-data/historical-data\n",
    "2. Replacing blanks with nans: https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas\n",
    "3. Setting/resetting index: https://sparkbyexamples.com/pandas/pandas-set-index-to-column-in-dataframe/\n",
    "4. Rainfall distribution: https://www.realclimate.org/index.php/archives/2017/11/a-brief-review-of-rainfall-statistics/. \n",
    "5. fitting distributions/picking distribution: https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python/16651955#16651955\n",
    "6. Exponential curve fitting:  https://www.geeksforgeeks.org/how-to-do-exponential-and-logarithmic-curve-fitting-in-python/\n",
    "7. Curve fitting: https://stackoverflow.com/questions/50448199/lognormal-curve-fit\n",
    "8. Curve fitting: https://www.geeksforgeeks.org/how-to-do-exponential-and-logarithmic-curve-fitting-in-python/\n",
    "9. Scipy curve fitting: https://stackoverflow.com/questions/2896179/fitting-a-gamma-distribution-with-python-scipy \n",
    "10. Scipy manual: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "11. Scipy curve fitting: https://glowingpython.blogspot.com/2012/07/distribution-fitting-with-scipy.html\n",
    "12. Wind speed PDF: https://www.sciencedirect.com/topics/engineering/wind-speed-distribution\n",
    "13. Wind speed PDF: https://www.frontiersin.org/articles/10.3389/fenrg.2021.769920/full\n",
    "14. Numpy generator exponential: https://numpy.org/devdocs/reference/random/generated/numpy.random.Generator.exponential.html\n",
    "15. Scipy stats details of PDF curve fitting: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit\n",
    "16. Scipy stats parameters v underlying mu and sigma for lognormal data generation: https://stackoverflow.com/questions/51609299/python-np-lognormal-gives-infinite-results-for-big-average-and-st-dev\n",
    "17. Multivariate data fit: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy.stats.multivariate_normal\n",
    "18. Multivariate ranom data generation: https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html\n",
    "19. Linear regression in scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
